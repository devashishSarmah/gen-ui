# Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=genui
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_DB=genui_dev

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password_here

# JWT Configuration
JWT_SECRET=your_jwt_secret_key_here_min_32_chars
JWT_EXPIRATION=24h

# Application Configuration
BACKEND_PORT=3000
FRONTEND_PORT=4200
FRONTEND_URL=http://localhost:4200
NODE_ENV=development

# OAuth Configuration (GitHub)
GITHUB_CLIENT_ID=your_github_client_id_here
GITHUB_CLIENT_SECRET=your_github_client_secret_here
# Note: GitHub callback is now dynamic (uses FRONTEND_URL + /auth/github/callback)

# OAuth Configuration (Google)
GOOGLE_CLIENT_ID=your_google_client_id_here
GOOGLE_CLIENT_SECRET=your_google_client_secret_here
# Note: Google callback is now dynamic (uses FRONTEND_URL + /auth/google/callback)
# Make sure your Google Cloud Console has the following redirect URI whitelisted:
# - http://localhost:4200/auth/google/callback (for local dev)
# - https://your-production-domain.com/auth/google/callback (for production)

# Google Analytics Configuration (Frontend)
GA_MEASUREMENT_ID=your_google_analytics_measurement_id_here

# AI Provider Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4.1-mini
# Optional tiered model overrides selected by RouterAgentService
OPENAI_MODEL_FAST=
OPENAI_MODEL_QUALITY=
OPENAI_MODEL_BALANCED=
# OpenAI Web Search (Responses API)
# Set to true to enable the web_search tool in OpenAIProvider.
OPENAI_WEB_SEARCH=false
# Comma-separated allowlist of domains (optional). Example: openai.com,docs.angular.io
OPENAI_WEB_SEARCH_ALLOWLIST=
# Set to false to disable live internet access (cache-only).
OPENAI_WEB_SEARCH_EXTERNAL=true
# Optional model for OpenAI web search tool calls (Responses API).
OPENAI_WEB_SEARCH_MODEL=gpt-5
# Web search orchestration mode: auto | always | never
AI_WEB_SEARCH_MODE=auto
# Optional comma-separated keywords to force search in auto mode.
AI_WEB_SEARCH_KEYWORDS=latest,today,current,news,price,release,version

ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-latest
ANTHROPIC_MODEL_FAST=
ANTHROPIC_MODEL_QUALITY=

OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=google/gemini-2.5-flash
OPENROUTER_MODEL_FAST=
OPENROUTER_MODEL_QUALITY=
# Optional explicit OpenRouter schema/router/repair model keys
OPENROUTER_SCHEMA_MODEL_BALANCED=google/gemini-2.5-flash
OPENROUTER_SCHEMA_MODEL_QUALITY=google/gemini-2.5-pro
OPENROUTER_ROUTER_MODEL_FAST=openai/gpt-4o-mini
OPENROUTER_REPAIR_MODEL_FAST=openai/gpt-4.1-mini
# Optional model override for the validation agent (defaults to OPENROUTER_MODEL)
OPENROUTER_VALIDATOR_MODEL=

# Gemini provider (OpenAI-compatible endpoint)
GEMINI_API_KEY=your_gemini_api_key_here
GOOGLE_API_KEY=
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
GEMINI_MODEL=gemini-2.5-flash
GEMINI_MODEL_PRO=gemini-2.5-pro
GEMINI_MODEL_FAST=gemini-2.0-flash
GEMINI_MODEL_QUALITY=gemini-2.5-pro
GEMINI_MODEL_FLASH=gemini-2.5-flash

# Groq provider (OpenAI-compatible endpoint)
GROQ_API_KEY=your_groq_api_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_MODEL_FAST=llama-3.1-8b-instant
GROQ_MODEL_QUALITY=llama-3.3-70b-versatile
GROQ_MODEL_ROUTER_FAST=llama-3.1-8b-instant
GROQ_MODEL_SUMMARIZER_FAST=llama-3.1-8b-instant
GROQ_MODEL_COPY_FAST=llama-3.1-8b-instant
GROQ_MODEL_REPAIR_FAST=llama-3.3-70b-versatile

# Agent layer routing (provider/model selection per layer+tier)
AI_LAYER_DEFAULT_PROVIDER=gemini
AI_LAYER_FALLBACK_PROVIDERS=groq,openrouter

# Router (fast + low-cost)
AI_LAYER_ROUTER_PROVIDER=groq
AI_LAYER_ROUTER_PROVIDER_FAST=groq
AI_LAYER_ROUTER_MODEL_FAST=${GROQ_MODEL_ROUTER_FAST}

# Summarizer (deterministic first, LLM fallback fast)
AI_LAYER_SUMMARIZER_PROVIDER=groq
AI_LAYER_SUMMARIZER_PROVIDER_FAST=groq
AI_LAYER_SUMMARIZER_MODEL_FAST=${GROQ_MODEL_SUMMARIZER_FAST}

# Copy (optional, fast)
AI_LAYER_COPY_PROVIDER=groq
AI_LAYER_COPY_PROVIDER_FAST=groq
AI_LAYER_COPY_MODEL_FAST=${GROQ_MODEL_COPY_FAST}

# UX planner (quality)
AI_LAYER_UX_PROVIDER=gemini
AI_LAYER_UX_MODEL_QUALITY=gemini-2.5-pro

# Schema generation/update
AI_LAYER_SCHEMA_PROVIDER=gemini
AI_LAYER_SCHEMA_PROVIDER_BALANCED=gemini
AI_LAYER_SCHEMA_PROVIDER_QUALITY=gemini
AI_LAYER_SCHEMA_MODEL_BALANCED=${GEMINI_MODEL_FLASH}
AI_LAYER_SCHEMA_MODEL_QUALITY=${GEMINI_MODEL_PRO}

# Repair: fast on Groq, quality escalation on Gemini
AI_LAYER_REPAIR_PROVIDER=groq
AI_LAYER_REPAIR_PROVIDER_FAST=groq
AI_LAYER_REPAIR_PROVIDER_QUALITY=gemini
AI_LAYER_REPAIR_MODEL_FAST=${GROQ_MODEL_REPAIR_FAST}
AI_LAYER_REPAIR_MODEL_QUALITY=${GEMINI_MODEL_PRO}

# Optional safety review model (if AI_ETHICS_LLM_REVIEW=true)
AI_LAYER_SAFETY_PROVIDER=groq
AI_LAYER_SAFETY_PROVIDER_FAST=groq
AI_LAYER_SAFETY_MODEL_FAST=${GROQ_MODEL_ROUTER_FAST}

# Optional per-layer fallback chains
AI_LAYER_ROUTER_FALLBACK_PROVIDERS=openrouter,gemini
AI_LAYER_SUMMARIZER_FALLBACK_PROVIDERS=openrouter,gemini
AI_LAYER_UX_FALLBACK_PROVIDERS=openrouter
AI_LAYER_SCHEMA_FALLBACK_PROVIDERS=openrouter,groq
AI_LAYER_REPAIR_FALLBACK_PROVIDERS=openrouter,gemini

# Router / summarizer / repair behavior
AI_ROUTER_MODE=hybrid
AI_SUMMARIZER_FALLBACK_LLM=true
AI_REPAIR_ESCALATE_TO_QUALITY=true

# Ethics and injection gate
AI_ETHICS_GATE_ENABLED=true
AI_ETHICS_BLOCK_PROMPT_INJECTION=true
AI_ETHICS_LLM_REVIEW=false

# Cost model (USD per 1K tokens) used for telemetry; keep zero for zero-cost setups
AI_COST_OPENAI_INPUT_PER_1K=0
AI_COST_OPENAI_OUTPUT_PER_1K=0
AI_COST_OPENROUTER_INPUT_PER_1K=0
AI_COST_OPENROUTER_OUTPUT_PER_1K=0
AI_COST_ANTHROPIC_INPUT_PER_1K=0
AI_COST_ANTHROPIC_OUTPUT_PER_1K=0
AI_COST_GEMINI_INPUT_PER_1K=0
AI_COST_GEMINI_OUTPUT_PER_1K=0
AI_COST_GROQ_INPUT_PER_1K=0
AI_COST_GROQ_OUTPUT_PER_1K=0
# Comma-separated tool allowlist for policy gate (required for tool.call actions)
AI_ALLOWED_TOOLS=
# Optional startup health-check toggles
AI_PROVIDER_HEALTHCHECK_ENABLED=true
